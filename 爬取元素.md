# 1. 正则表达式:

规则:

| 符号 | 含义 |
| ---- | ---- |
| . | 匹配除换行符以外的字符 |
| [ ] | [aoe] [a-w] 匹配集合中任何一个字符 |
| \d | 数字[0-9] |
| \D | 非数字 |
| \w | 数字\字母\下划线\中文 |
| \W | 非\w |
| \s | 所有空白字符 |
| \S | 所有非空白字符 |



| 数量修饰 | 含义         |
| -------- | ------------ |
| *        | 0或多次>=0   |
| +        | 1次或多次>=1 |
| ?        | 0次或1次     |
| {m}      | 固定m次      |
| {m,}     | 至少m次      |
| {m,n}    | m次到n次     |

| 边界 | 含义             |
| ---- | ---------------- |
| $    | 匹配字符串的结尾 |
| ^    | 匹配字符串的开头 |
| \b   | 匹配单词边界     |
| \B   | 匹配非单词边界   |

~~~bash
1. (ab){3} 匹配ababab => 视为一个整体
2. 子模式:!!!!!!!
~~~

~~~python
import re
string = '<p><div><span>猪八戒</span></div></p>'
pattern = re.comile(r'<(/w+)><(\w+)>\w+</\2></\1>')
#r 可以使子字符串不被转义, \t \w保持原本样子
# \1 表示匹配到的第一个子模式,  \2表示第二个
# 这样可以保证匹配的更准确

ret=pattern.search(string)
使用子模式返回列表

~~~

| 模式 | 含义                       |
| ---- | -------------------------- |
| .+?  | 非贪婪模式(尽可能少的匹配) |
| re.I | 忽略大小写(ignore) |
| re.M | 多行匹配(multiple lines) |
| re.S | 单行匹配(single line) |

~~~python
pattern = re.compile(r'', re.M)
pattern = re.compile(r'', re.S)
pateern = re.compile(r'',re.I)
~~~

~~~mar
注意:\w+ 不能匹配换行符,被截断,如果为re.S, 将匹配失败
  只用 .*  也无法全部匹配,  但如果为re.S, .* 将也匹配换行符
~~~

| 匹配方法 | 说明                                     |
| -------- | ---------------------------------------- |
| match    | 从开头开始找第一个匹配的,返回一个Matcher |
| search   | 从任意位置找第一个匹配的                 |
| findall  | 找到所有匹配的元素并返回列表             |

返回列表,列表中可能是元组,看你是否用了子匹配,如果用了一个,就返回子匹配的结果, 如果没用,就返回匹配的整个串, 然后所有串组成列表

具体group是如何体现的,写代码就知道了

~~~python
m=pat.match(..)
m.group(0) #返回整个匹配结果
m.group(1) #返回匹配的第一个字串
~~~



| 正则函数 | 函数原型                                                 | 功能                                      |
| -------- | -------------------------------------------------------- | ----------------------------------------- |
| sub      | pat.sub(substr, string)  re.sub(pattern, substr, string) | 将string中与pattern匹配的字串用substr替换 |

~~~http
注意:substr 可以用一个函数代替,函数需要返回一个str类型的值,其参数为pattern对象本身, 需要将匹配的内容提取出来 
~~~



~~~python
string = 'i love you, you love me, ye"
pattern = re.compile(r'love')
ret = pattern.sub('hate',string)
#ret = re.sub(pattern, substr, string)
print(ret)
#i hate you, you hate me, ye
~~~

# 2. BS4使用:(Beautiful Soup 4库)

{需要国内源才能装上, bs4 依赖一个库 -> lxml} = > pip3 install  bs4

~~~python
from bs4 import BeautifulSoup
~~~

1. 查找本地文件:

~~~python
soup = BeautifulSoup(open("<local-file>",charset=""),'lxml')
~~~

2. 转化网络文件:

~~~python
soup = BeatuifulSoup(<内容>,'解析器')
#内容可以是字节类型也可以是字符类型,然而有时候某种类型可能会出问题,需要转化类型.
~~~

3. bs4的使用

~~~python
#bs4中所有对象都有完备的__str__()实现
#直接通过标签名查找,获取第一个符合要求的tag节点
soup.div  

#查找标签的属性
soup.a['href']
soup.div['title']
soup.a.attrs #返回一个字典,key为属性名,value为属性值
soup.a.attrs['href']

#获取标签内容
soup.a.text  #获取标签内以及子标签的所有文本内容
soup.a.string  #只能获取到标签内文本内容,如果还有子标签则无法获取
soup.a.get_text()   #获取到标签以及子标签的所有文本内容

#查找标签
soup.find('a')  #与上面的查找一致,只获取第一个满足条件的标签
soup.find('a',title="book")
soup.find('a',class_="du")  #class在python中是关键字!

#高级的功能
div = soup.find('div',class_="tang")
a = div.find('a',class_='du')

#查找全部
all_a = soup.find_all("div")
all_a_b = soup.find_all(['a','b'])   #找到所有a和b标签节点
all_a_limit2 = soup.find_all("div",limit=2)  #只查找前2个

#select 根据选择器选择内容
soup.select('.tang > div > ui>li > a')   #返回一个列表,里面是满足条件的节点
#!!选择器返回的都是**列表**!!无论几个元素
~~~

# 3.xpath使用

​		***{xpath 是一门在xml文档中查找信息的语言,是一种路径表达式,我们使用Xpath封装后的,来搜索HTML内容}*** 

| 路径表达式 | 作用                           |
| ---------- | ------------------------------ |
| nodename   | 获取nodename节点下的所有子节点 |
| //         | 不考虑位置,从任意位置开始找    |
| /          | 从文档根目录开始查找           |
| .          | 在当前节点查找                 |
| ..         | 当前节点的父节点               |
| @          | 选取属性                       |

~~~xquery
1. /bookstore/book  直接子节点book
2. //book		所有book
3. /bookstore//book   选取bookstroe下所有book
!!! /bookstore//book/text()  获取book节点的文本内容
!!! /bookstore//book//text()  获取book节点下所有节点的文本内容
!!! /bookstore//book/@href  获取book节点的href属性内容
~~~

* ***谓词*** 

~~~js
/bookstore/book[1] 第一个元素
/bookstore/book[last()]  最后一个元素
/bookstore/book[position()<3]  前两个元素
/bookstore[@lang]   拥有lang属性的boostore节点
/bookstrore[@lang="eng"]  属性lang的值为"eng"的节点
/bookstrore[@lang="eng" and @name="liang"]  属性lang的值为"eng" 并且的节点名称为"liang"的
/bookstore/book[price>35]  book中有price子节点,且值大于35的book节点将被选择
~~~

*  ***通配符***

~~~xml
* 匹配任何元素节点
@* 匹配任何属性节点
node() 匹配任何类型的node
~~~

* ***运算符***

~~~c++
//book/title | //book/price 两种类型的节点都被选取
// + 加法
// - 减法
// * 乘法
// div 除法
// = 等于(逻辑判断)
// != 不等于
// < 小于
// <= 小于或等于
// > 大于
// >=大于或等于
// or 或
// and  与
// mod 取余
~~~

* ***模糊匹配***

~~~xquery
//input[contains(@class,"s")]
<!--input中有class属性, class属性有"s"字串-->

//input[starts-with(@class,"s")]
<!--input中有class属性,class属性以"s"开头-->
~~~

* ***使用方法: 去谷歌浏览器中下载Xpath Helper并安装,就可以在浏览器中使用XPath了***

* ***代码中使用XPath***

~~~python
from lxml import etree
#注意xml语法和xhmtl类似, 语法都更加严格,很可能会报错
#两种使用方式
# 将html文档变成一个对象,然后调用方法去查找指定的节点
# 1. 本地文件(一般通过操作本地文件来学习
tree = etree.parse(<file-name>)
ret = tree.xpath("//div[@class='tang']/ul/li[1]/text()")
#注意xpath函数返回的无论如何都是列表!!!!!
#同时xpath中第一个元素是1开始,而python中是0开始!
ret1 = tree.xpath("//div[@class='tang']/ul/li[1]")
print(ret1[0].text)
#搜索内容中含有"爱"的li节点
ret2 = tree.xpath("//li[contains(text(),"爱")]")
# 2. 网络文件
tree = etree.HTML(<网页内容>)
~~~

~~~http
1.text() 方法返回内容时,除了不能获取子节点的子节点的文本,还有个很大的问题,即会有\t\n这些特殊字符
2. 有个很好用的方法解决上面问题
strResult = ret[0].xpath("string(.)") #将当前节点字符串化
#string()的作用即将当前节点以及子孙节点的内容拼接起来,但是有空格和大量的\t以及\n
strResult.replace("\t","").replace("\n","");
#尽管strResult并非str类型, 而是一种Element文本类型,但是可以调用str的方法!
~~~

# 4.图片懒加载

为了避免同时加载大量图片给服务器带来大量压力, 图片的src设置为同一个图片, 并自定义一个属性:data-**, 如data-src , 当图片可以看到时, 通过js调用, 使之被加载. 缓解服务器压力.

通过爬虫爬取懒加载图片时, 是无法让懒加载图片加载真实图片的***(即发现src是错的,或者图片根本没有src这个属性)***

~~~http
再注意:jquery中选取元素的属性, 都是在html页面中的属性,如果在html没有这个属性,而是在js中设置的,很可能取不到这个元素. 但是如果在html页面中设置了该属性, js修改其值也没关系,后面获取值时获取的是该属性真实的值.
唯一的问题在于: 如果html页面中无该属性名, 取值时将出现问题! 尤其是第一次取值时!
而在爬虫中,js的修改看不到!(而在网页的页面源码中可以看到)=>导致chrome中xpath和代码的xpath结果可能不同
(故而网页源码中的元素属性并非是原生的,很可能不能通过xpath选择到)
!!!{另一方面, 自定义的元素在网页元素中一般不会显示出来}
~~~

# 5.JSONPath

1. python处理json函数用到的函数

~~~pythopn
import json
json.dumps() #将字典或列表转化为json格式的字符串
json.loads() #将json格式字符串转化为python对象
json.dump()  #将字典或列表转化为json格式字符串并且写入文件中
json.load()  #从文件中读取json格式字符串,转化为python对象
~~~

	2. 前端处理json

~~~js
JSON.parse(<json字符串>);  //转化为json对象
JSON.stringfy(<json对象>);  //转为json字符串
eval(<json格式字符串>);  //转为json,更不专业
~~~

3. jsonpath

~~~python
import jsonpath
~~~

| JSONPath   | 作用             |
| ---------- | ---------------- |
| $          | 根节点           |
| @          | 现行节点         |
| .      [ ] | 取子节点         |
| *          | 匹配所有元素节点 |
| [ ]        | 取下标(简单迭代) |
| [ , ]      | 迭代器中多选     |
| ?()        | 过滤操作         |
| ()         | 支持计算表达式   |
| ..         | 类似 //的作用!   |

| JSONPath               | 结果                 |
| ---------------------- | -------------------- |
| $.store.book[*].author | 商店里所有书籍的作者 |
| $..author              | 所有作者             |
| $.store.*              | 商店里所有东西       |
| $.store..price         | 商店里一切的价格     |
| $..book[2]             | 第三本书             |
| $..book[(@.length-1)]  | 最后一本书           |
| $..book[-1:]           | 最后一本书           |
| $..book[0,1]           | 前两本书             |
| $..book[:2]            | 前两本书             |
| $..book[?(@.isbn)] | 筛选具有isbn的书籍 |
| $..book[?(@.price<10)] | 帅选价格小于10的书籍 |

~~~js
xpath 索引从1开始
jsonpath 索引从0开始
~~~

~~~python
import jsonpath
import json
obj = json.load(open("my.json","r",encoding="utf-8"))
jsonpath.jsonpath(obj,'$..book[*].author') 
#此处book的值时数组,要获取数组中所有的值,必须取下标 book[*],如果不用[]取任何一个下标,是取不到元素的, 从而返回的是个False, 用book[0]最后也能返回一个值
~~~








